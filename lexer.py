import re


class Lexer:
    def __init__(self, source_code):
        # Store the source code string to be tokenized
        self.source_code = source_code
        # List to hold the generated tokens
        self.tokens = []
        # Current position in the source code string
        self.current_position = 0

    def tokenize(self):
        # Define the types of tokens and their regex patterns
        token_specification = [
            ('NUMBER', r'\d+'),  # Integer numbers
            ('KEYWORD', r'\b(lambda|if|else|return)\b'),  # Keywords
            ('ID', r'[A-Za-z_]\w*'),  # Identifiers (names of variables or functions)
            ('OP', r'[+\-*/%]'),  # Arithmetic operators
            ('LPAREN', r'\('),  # Left parenthesis
            ('RPAREN', r'\)'),  # Right parenthesis
            ('BOOLEAN', r'(True|False)'),  # Boolean values
            ('COMPARE', r'==|!=|<=|>=|<|>'),  # Comparison operators
            ('NEWLINE', r'\n'),  # Newline characters
            ('SKIP', r'[ \t]+'),  # Spaces and tabs
            ('DELIM', r'[,]'),  # Comma delimiter
            ('COLON', r':'),  # Colon delimiter
            ('LOGICAL', r'(&&|\|\|)'),  # Logical operators
            ('MISMATCH', r'.'),  # Any other character (for error handling)
        ]

        # Combine all regex patterns into a single pattern
        tok_regex = '|'.join(f'(?P<{pair[0]}>{pair[1]})' for pair in token_specification)
        # Compile the regex pattern into a regex object
        get_token = re.compile(tok_regex).match
        # Initialize the current position in the source code string
        line = self.source_code
        pos = 0
        # Match the first token in the source code
        mo = get_token(line)

        # Loop through the source code and match all tokens
        while mo is not None:
            typ = mo.lastgroup  # Get the type of the current token
            if typ == 'NUMBER':
                value = int(mo.group(typ))  # Convert number string to integer
            elif typ == 'BOOLEAN':
                value = mo.group(typ) == 'True'  # Convert boolean string to True/False
            elif typ == 'ID' and mo.group(typ) in ('True', 'False'):
                typ = 'BOOLEAN'
                value = mo.group(typ) == 'True'
            elif typ == 'SKIP':
                pos = mo.end()  # Skip whitespace and update position
                mo = get_token(line, pos)  # Get the next match
                continue
            elif typ == 'MISMATCH':
                # Raise an error if an unexpected character is found
                raise RuntimeError(f'Unexpected character {mo.group(typ)} at position {pos}')
            else:
                value = mo.group(typ)  # Get the value of the current token
            # Add the token type and value to the list of tokens
            self.tokens.append((typ, value))
            pos = mo.end()  # Update the current position
            mo = get_token(line, pos)  # Get the next match

        # Raise an error if there are any unprocessed characters left
        if pos != len(line):
            raise RuntimeError(f'Unexpected character {line[pos]} at position {pos}')
        return self.tokens  # Return the list of tokens


if __name__ == '__main__':
    # Create a lexer with the input string
    lexer = Lexer("lambda x, y: x + y == 8")
    # Print the list of tokens generated by the lexer
    print(lexer.tokenize())
